# Model arguments
model_name_or_path: neulab/Pangea-7B
dataset_name: CNCL-Penn-State/cpo_en_multitask_text_fa_msd5_mm10_ms20_div_nov_sur_qua_test
dataset_train_split: val_sample1024
dataset_test_split: val_sample1024
output_dir: /mnt/scratch/home/ismayilz/models/test-rm-pangea-7b-lora

# Training arguments
per_device_train_batch_size: 4
per_device_eval_batch_size: 32
gradient_accumulation_steps: 8
num_train_epochs: 1
learning_rate: 1.0e-4
lr_scheduler_type: cosine
weight_decay: 1.0e-3
logging_steps: 4
eval_strategy: steps
eval_steps: 8
eval_on_start: true
save_strategy: steps
save_steps: 128
save_total_limit: 1
max_length: 1024
use_peft: true
lora_r: 32
lora_alpha: 16
lora_task_type: SEQ_CLS
report_to: none
run_name: test-rm-pangea-7b-lora
dataloader_pin_memory: true
dataloader_num_workers: 4
dataloader_prefetch_factor: 2
resume_from_checkpoint: true
metric_for_best_model: accuracy
greater_is_better: true
seed: 42
fp16: true
add_margin_loss: true
lora_target_modules:
  - v_proj
  - q_proj